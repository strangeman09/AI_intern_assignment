{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport torch\nimport datasets\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-06T17:22:59.706826Z","iopub.execute_input":"2023-10-06T17:22:59.707203Z","iopub.status.idle":"2023-10-06T17:23:05.681837Z","shell.execute_reply.started":"2023-10-06T17:22:59.707179Z","shell.execute_reply":"2023-10-06T17:23:05.680743Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = \"t5-small\"\nconfig = AutoConfig.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)  \nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name, config=config)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-06T17:23:05.683654Z","iopub.execute_input":"2023-10-06T17:23:05.684202Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c63bbc746af041c68d1e7f626de25b17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7c6602050eb424fbd904f61372813b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"988c3353ec7d42aba3bb2a2618ffb47a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dafcead335274f809e9b8afca40b95b5"}},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer\n\ndataset = load_dataset('findnitai/english-to-hinglish')\n\nmaster = []\nfor line in dataset['train']['translation']:\n    master.append(line['en'])\n    master.append(line['hi_ng'])\n\ndef gen_training_data():\n    return (master[i : i+500] for i in range(0, len(master), 500))\n\ntokenizer_training_data = gen_training_data()\ntokenizer = tokenizer.train_new_from_iterator(tokenizer_training_data, 32128) ","metadata":{"execution":{"iopub.status.idle":"2023-10-06T17:23:35.768944Z","shell.execute_reply.started":"2023-10-06T17:23:17.696327Z","shell.execute_reply":"2023-10-06T17:23:35.767803Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset['train']","metadata":{"execution":{"iopub.status.busy":"2023-10-06T17:23:35.771590Z","iopub.execute_input":"2023-10-06T17:23:35.771922Z","iopub.status.idle":"2023-10-06T17:23:35.778717Z","shell.execute_reply.started":"2023-10-06T17:23:35.771891Z","shell.execute_reply":"2023-10-06T17:23:35.777580Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['translation'],\n    num_rows: 189102\n})"},"metadata":{}}]},{"cell_type":"code","source":"source_prefix = \"Translate English to Hinglish : \"\nsource_lang = \"en\"\ntarget_lang = \"hi_ng\"\nmax_source_length = 128 \nmax_target_length = 128 \npadding = \"max_length\" \nnum_epochs = 1","metadata":{"execution":{"iopub.status.busy":"2023-10-06T17:23:35.779997Z","iopub.execute_input":"2023-10-06T17:23:35.781073Z","iopub.status.idle":"2023-10-06T17:23:35.787699Z","shell.execute_reply.started":"2023-10-06T17:23:35.781018Z","shell.execute_reply":"2023-10-06T17:23:35.786871Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def preprocess(source_data):\n    inputs = [sample[source_lang] for sample in source_data[\"translation\"]]\n    targets = [sample[target_lang] for sample in source_data[\"translation\"]]\n    inputs = [source_prefix + inp for inp in inputs]\n    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n    \n   \n    labels = tokenizer(text_target=targets, max_length=max_target_length, padding=padding, truncation=True)\n    \n   \n    labels[\"input_ids\"] = [\n        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n    ]\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    \n    return model_inputs\n","metadata":{"execution":{"iopub.status.busy":"2023-10-06T17:23:35.789271Z","iopub.execute_input":"2023-10-06T17:23:35.789902Z","iopub.status.idle":"2023-10-06T17:23:35.802475Z","shell.execute_reply.started":"2023-10-06T17:23:35.789858Z","shell.execute_reply":"2023-10-06T17:23:35.801513Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_dataset = dataset[\"train\"]\ntrain_dataset = train_dataset.map(preprocess, batched=True, remove_columns=\"translation\")","metadata":{"execution":{"iopub.status.busy":"2023-10-06T17:23:35.803725Z","iopub.execute_input":"2023-10-06T17:23:35.804735Z","iopub.status.idle":"2023-10-06T17:24:31.279829Z","shell.execute_reply.started":"2023-10-06T17:23:35.804704Z","shell.execute_reply":"2023-10-06T17:24:31.278830Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/190 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77c725aa44a1421ca21a6d34f9ea98f5"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import HfArgumentParser\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\ntrainer_args_in = {\n    'output_dir': 't5-hinglish-translator',\n    'overwrite_output_dir' : True,\n    'do_train' : True,\n   \n    'per_device_train_batch_size' : 32,\n    'num_train_epochs' : num_epochs,\n    'report_to': 'none',\n    'save_total_limit':1\n}\n\nparser = HfArgumentParser((Seq2SeqTrainingArguments))\ntraining_args = parser.parse_dict(trainer_args_in)\n\ntrainer = Seq2SeqTrainer(model=model, args=training_args[0], train_dataset=train_dataset, tokenizer=tokenizer)\n\ntrain_result = trainer.train(resume_from_checkpoint=None)","metadata":{"execution":{"iopub.status.busy":"2023-10-06T17:24:31.281309Z","iopub.execute_input":"2023-10-06T17:24:31.282008Z","iopub.status.idle":"2023-10-06T17:56:02.613253Z","shell.execute_reply.started":"2023-10-06T17:24:31.281975Z","shell.execute_reply":"2023-10-06T17:56:02.612286Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5910' max='5910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5910/5910 31:23, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>5.535600</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>4.634000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>4.249400</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>4.007700</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>3.823600</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>3.699700</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>3.594500</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>3.489300</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>3.463700</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>3.372200</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>3.388300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"device='cuda' if torch.cuda.is_available else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-10-06T17:57:05.446330Z","iopub.execute_input":"2023-10-06T17:57:05.446664Z","iopub.status.idle":"2023-10-06T17:57:05.452789Z","shell.execute_reply.started":"2023-10-06T17:57:05.446638Z","shell.execute_reply":"2023-10-06T17:57:05.451823Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"input_ids = tokenizer(\"translate English to Hinglish: what is your name brother?\", return_tensors=\"pt\").input_ids\noutputs = model.generate(input_ids.to(device))\nprint(\"Test Output : \" + tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2023-10-06T17:57:29.741410Z","iopub.execute_input":"2023-10-06T17:57:29.741765Z","iopub.status.idle":"2023-10-06T17:57:29.801252Z","shell.execute_reply.started":"2023-10-06T17:57:29.741738Z","shell.execute_reply":"2023-10-06T17:57:29.800259Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Test Output : your name ko hai\n","output_type":"stream"}]}]}